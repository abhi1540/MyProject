{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7a7c56-8c8b-453f-a7d4-d2bbde89afdf",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "    **Implement BLEU Score metric. Pre-process the text by lower-casing the text and removing punctuation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd7f26e-8379-488e-884a-35d4090698dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class BleuTextEvaluator:\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses text by lowercasing and removing punctuation.\n",
    "        Args:\n",
    "            text: The text to preprocess.\n",
    "        Returns:\n",
    "            The preprocessed text.\n",
    "        \"\"\"\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "        # Remove punctuation\n",
    "        text = ''.join([char for char in text if char.isalnum() or char == ' '])\n",
    "        return text\n",
    "\n",
    "    def calculate_modified_precision(self, reference_sentence, generated_sentence, n):\n",
    "        \"\"\"\n",
    "        Calculate modified precision of n-grams between a reference sentence\n",
    "        and a generated sentence.\n",
    "\n",
    "        Args:\n",
    "        reference_sentence (str): The reference sentence.\n",
    "        generated_sentence (str): The generated sentence to be evaluated.\n",
    "        n (int): The size of the n-grams.\n",
    "\n",
    "        Returns:\n",
    "        float: Modified precision score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize reference and generated sentences\n",
    "        reference_tokens = self.preprocess_text(reference_sentence).split()\n",
    "        generated_tokens = self.preprocess_text(generated_sentence).split()\n",
    "\n",
    "        # Create n-grams for reference and generated sentences\n",
    "        reference_ngrams = [tuple(reference_tokens[i:i+n]) for i in range(len(reference_tokens)-n+1)]\n",
    "        generated_ngrams = [tuple(generated_tokens[i:i+n]) for i in range(len(generated_tokens)-n+1)]\n",
    "\n",
    "        # Count n-grams occurrences\n",
    "        reference_counts = Counter(reference_ngrams)\n",
    "        generated_counts = Counter(generated_ngrams)\n",
    "\n",
    "        # Calculate clipped counts and total counts\n",
    "        clipped_counts = sum(min(generated_counts[ngram], reference_counts[ngram]) for ngram in generated_counts)\n",
    "        total_counts = sum(generated_counts.values())\n",
    "\n",
    "        # Handle division by zero\n",
    "        if total_counts == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Calculate modified precision\n",
    "        precision = clipped_counts / total_counts\n",
    "\n",
    "        return precision\n",
    "\n",
    "    def calculate_bleu_score(self, references, candidates, N):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for a list of reference sentences and a list of candidate sentences.\n",
    "\n",
    "        Args:\n",
    "        references (list of str): List of reference sentences.\n",
    "        candidates (list of str): List of candidate sentences to be evaluated.\n",
    "        N (int): The size of the n-grams.\n",
    "\n",
    "        Returns:\n",
    "        float: BLEU score.\n",
    "        \"\"\"\n",
    "        brevity_penalty = 1\n",
    "        bleu_scores = []\n",
    "        for ref, candidate in zip(references, candidates):\n",
    "            weighted_precision_scores = sum((1/N) * np.log(self.calculate_modified_precision(ref, candidate, int(n_gram)) + \\\n",
    "                                                           10**-6 # Added small noise to avoid numerical instability\n",
    "                                                          ) for n_gram in range(1,N+1))\n",
    "            bleu_scores.append(brevity_penalty * np.exp(weighted_precision_scores))\n",
    "        return bleu_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b6c10-d886-472c-a71a-eceeda9c2057",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "        **Use this implementation to find BLEU Score when x=\"The boys were playing happily on theground.\" and y=\"The boys were playing football on the field.\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0804770a-8737-4871-a9f4-e60b78cc1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Scores: 0.41113\n"
     ]
    }
   ],
   "source": [
    "text_evaluator = BleuTextEvaluator()\n",
    "references = [\n",
    "    \"the boys were playing football on the field.\"\n",
    "]\n",
    "candidates = [\n",
    "    \"the boys were playing happily on the ground.\"\n",
    "]\n",
    "# Size of N-gram\n",
    "N = 4\n",
    "bleu_scores = text_evaluator.calculate_bleu_score(references, candidates, N)\n",
    "print(\"BLEU Scores: {:.5f}\".format(bleu_scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab932dd-ec2e-46fc-8c22-b8eb92b92a29",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "    **Can you explain why we are taking minimum in numerator in equation 1?**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1862d010-d806-4eb1-8d19-caa46a3a03c2",
   "metadata": {},
   "source": [
    "The min(generated_counts[ngram], reference_counts[ngram]) part ensures that the numerator (number of correct n-grams)\n",
    "in the modified precision formula doesn't exceed the number of times that n-gram appears in the reference sentence,\n",
    "even if it appears more frequently in the generated sentence.This effectively clips the candidate n-gram count to the\n",
    "maximum supported count by the reference sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a982e-bb86-4861-8c77-971eff0c9ed0",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "    **Use your implementation to find BLEU Score between any 5 sentence pairs and explain what are potential disadvantages of using the BLEU Score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56aa751a-b9ac-4d81-9fb6-b03d9fcca850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score 0.7598\n",
      "BLEU Score 1.0000\n",
      "BLEU Score 0.0008\n",
      "BLEU Score 0.0191\n",
      "BLEU Score 0.0161\n"
     ]
    }
   ],
   "source": [
    "text_evaluator = BleuTextEvaluator()\n",
    "references = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog barked loudly\",\n",
    "    \"the moon shining is\",\n",
    "    \"birds are chirping in the trees\",\n",
    "    \"a delicious meal is being prepared\"\n",
    "]\n",
    "\n",
    "candidates = [\n",
    "    \"a cat sat on the mat\",\n",
    "    \"the dog barked loudly\",\n",
    "    \"the moon is shining\",\n",
    "    \"birds chirp in the trees\",\n",
    "    \"a tasty meal is being cooked\"\n",
    "]\n",
    "N = 4\n",
    "bleu_scores = text_evaluator.calculate_bleu_score(references, candidates, N)\n",
    "for score in bleu_scores:\n",
    "    print(\"BLEU Score {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8cb13-1d21-403e-ad47-199c8ae22ff7",
   "metadata": {},
   "source": [
    "- BLEU relies solely on n-gram precision, which may not capture the semantic similarity between sentences accurately. It doesn't consider word order or sentence structure, leading to potentially misleading scores.\n",
    "- BLEU tends to favor shorter translations due to the brevity penalty. Longer reference sentences may penalize translations unfairly, especially if they contain additional information not present in the translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
