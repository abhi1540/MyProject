Below are NLP Quiz Questions:
Quiz 1:
1. [2 Marks] Define the RTE task from GLUE Benchmark.
2. [2 Marks] Mention two differences between BERT and ELMO models
3. [4 Marks] In Transformer Encoder-Decoder architecture, why is there an Encoder-Decoder Cross attention in the Decoder block, but this cross-attention is not there in the Encoder Block?
4. [2 Marks] Why is the attention function used in transformer architecture called scaled dot product attention?

Quiz 2:
1. [2 Marks] What is the difference between anaphora resolution and cataphora resolution?
2. [2 Marks] Mention pair algorithms for coreference resolution check if a mention pair is a coreference. However, performing this for a large number of mention pairs may be time consuming. Suggest two rule based methods to filter some of the mention pairs which will not be coreferences.
3. [3 Marks] Suppose you are designing a feature-based algorithm, that, given a candidate antecedent mention and an anaphor mention, finds out whether they are coreferences or not. Suggest six features for that the algorithm can use.
4. [3 Marks] What is the Winograd schema challenge problem? Given an example instance for this task. The example should be your own example, something that is not covered in the class.

Quiz 3:
1. [3 Marks] Describe coherence in your own words. Also provide an example.
2. [3 Marks] What type of discourse connectors exist in the following text? "The bank released all the bond details today. It was asked by the court to do so."
3. [2 Marks] Give a problem statement (in a formal way) for describing the coherence estimation using sentence order reconstruction strategy.
4. [2 Marks] Mention two strategies for reducing the number of spans /mentions to be considered in coreference resolution algorithm.
